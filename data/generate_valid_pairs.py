import argparse
import os
import gc
import pickle
import pandas as pd
import numpy as np
from matchms.Spectrum import Spectrum
from matchms.filtering.metadata_processing.add_fingerprint import add_fingerprint
from matchms.similarity import FingerprintSimilarity, ModifiedCosine, CosineGreedy
from matchms.filtering import remove_peaks_around_precursor_mz, remove_peaks_outside_top_k
from matchms.filtering.metadata_processing.derive_inchi_from_smiles import derive_inchi_from_smiles 
from matchms.filtering.metadata_processing.derive_inchikey_from_inchi import derive_inchikey_from_inchi
from pandarallel import pandarallel
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
from joblib import Parallel, delayed

def produce_summary_plots(csv_path, output_dir):
    """This function summarizes the pairs generated by the generate_pairs function."""
    
    df = pd.read_csv(csv_path)
    
    
    # Plot the correlation between modified cosine and structural similarity
    plt.figure(figsize=(10, 10))
    plt.scatter(df['modified_cosine'], df['structural_similarity'])
    plt.xlabel('Modified Cosine')
    plt.ylabel('Structural Similarity')
    plt.title('Modified Cosine vs Structural Similarity')
    plt.savefig(os.path.join(output_dir, 'modified_cosine_vs_structural_similarity.png'))
    
    # Plot the correlation between greedy cosine and structural similarity
    plt.figure(figsize=(10, 10))
    plt.scatter(df['greedy_cosine'], df['structural_similarity'])
    plt.xlabel('Greedy Cosine')
    plt.ylabel('Structural Similarity')
    plt.title('Greedy Cosine vs Structural Similarity')
    plt.savefig(os.path.join(output_dir, 'greedy_cosine_vs_structural_similarity.png'))
    
    # 3D Plot of precursor mass difference, modified cosine and structural similarity
    fig = px.scatter_3d(df, x='precursor_mass_difference', y='modified_cosine', z='structural_similarity', color='precursor_mass_difference')
    fig.update_layout(
        scene=dict(
            xaxis_title='Precursor Mass Difference',
            yaxis_title='Modified Cosine',
            zaxis_title='Structural Similarity',
            )
    )
    fig.write_image(os.path.join(output_dir, 'precursor_mass_difference_vs_modified_cosine_vs_structural_similarity.png'))
    fig.write_html(os.path.join(output_dir, 'precursor_mass_difference_vs_modified_cosine_vs_structural_similarity.html'))
    fig.show()
    
    # 3D Plot of precursor mass difference, modified cosine, and matched peaks
    fig = px.scatter_3d(df, x='structural_similarity', y='modified_cosine', z='matched_peaks', color='structural_similarity')
    fig.update_layout(
        scene=dict(
            xaxis_title='Structural Similarity',
            yaxis_title='Modified Cosine',
            zaxis_title='Matched Peaks',
            )
    )
    fig.write_image(os.path.join(output_dir, 'structural_similarity_vs_modified_cosine_vs_matched_peaks.png'))
    fig.write_html(os.path.join(output_dir, 'structural_similarity_vs_modified_cosine_vs_matched_peaks.html'))
    
    # Plot modified cosine vs structural similarity,
    # Require 6 matches peaks
    df_copy = df.copy(deep=True)
    df_copy.loc[df_copy['matched_peaks'] < 6, 'modified_cosine'] = 0
    plt.figure(figsize=(10, 10))
    plt.scatter(df_copy['modified_cosine'], df_copy['structural_similarity'], alpha=0.10)
    # Plot y=x line
    plt.plot([0, 1], [0, 1], color='black')
    plt.xlabel('Modified Cosine')
    plt.ylabel('Structural Similarity')
    plt.title('Modified Cosine vs Structural Similarity (Matched Peaks >= 6, Modified Cosine Tol=0.5)')
    plt.savefig(os.path.join(output_dir, 'modified_cosine_vs_structural_similarity_matched_peaks_6.png'))
    
    # Create the     plot as shown in "Comparison of Cosine, Modified Cosine, and Neutral Loss Based Spectrum Alignment 
    # For Discovery ofStructurally Related Molecules"
    # Bin the data by structural similarity
    bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
    labels = ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']
    df['structural_similarity_bins'] = pd.cut(df['structural_similarity'], bins=bins, labels=labels)
    
    def _adjacent_values(vals, q1, q3):
        upper_adjacent_value = q3 + (q3 - q1) * 1.5
        print("upper_adjacent_value", upper_adjacent_value)
        print(vals[-1])
        upper_adjacent_value = np.clip(upper_adjacent_value, q3, vals[-1])

        lower_adjacent_value = q1 - (q3 - q1) * 1.5
        lower_adjacent_value = np.clip(lower_adjacent_value, vals[0], q1)
        return lower_adjacent_value, upper_adjacent_value
    
    data = [df.loc[df['structural_similarity_bins'] == i, 'modified_cosine'] for i in labels]
    data = [sorted(d) for d in data]
    
    # Create the violin plot
    plt.figure(figsize=(10, 10))
    plot = plt.violinplot(data, showmeans=False, showextrema=False, )
    for pc in plot['bodies']:
        pc.set_facecolor('blue')
        pc.set_edgecolor('black')
        pc.set_alpha(0.5)
        
    quartile1, medians, quartile3 = [],[],[]
    for lst in data:
        q1, m, q2 = np.percentile(lst, [25, 50, 75])
        quartile1.append(q1)
        medians.append(m)
        quartile3.append(q2)
        
    print(quartile1, medians, quartile3)
    print(len(quartile1), len(medians), len(quartile3), len(data))

    whiskers = np.array([ _adjacent_values(sorted_array, q1, q3) for sorted_array, q1, q3 in zip(data, quartile1, quartile3) ])
    whiskers_min, whiskers_max = whiskers[:, 0], whiskers[:, 1]

    inds = np.arange(1, len(medians) + 1)
    plt.scatter(inds, medians, marker='o', color='white', s=30, zorder=3)
    plt.vlines(inds, quartile1, quartile3, color='k', linestyle='-', lw=5)
    plt.vlines(inds, whiskers_min, whiskers_max, color='k', linestyle='-', lw=1)
        
    plt.xticks(range(1, len(labels)+1), labels)
    plt.xlabel('Structural Similarity')
    plt.ylabel('Modified Cosine')
    plt.title('Modified Cosine vs Structural Similarity')
    plt.savefig(os.path.join(output_dir, 'modified_cosine_vs_structural_similarity_violin.png'))
    
    # # Create the violin plot
    # plt.figure(figsize=(10, 10))
    # plt.violinplot([df.loc[df['structural_similarity_bins'] == i, 'modified_cosine'] for i in labels], showmeans=True)
    # plt.xticks(range(1, len(labels)+1), labels)
    # plt.xlabel('Structural Similarity')
    # plt.ylabel('Modified Cosine')
    # plt.title('Modified Cosine vs Structural Similarity')
    # plt.savefig(os.path.join(output_dir, 'modified_cosine_vs_structural_similarity_violin.png'))
    
    # Plot a histogram of the structural similarity
    plt.figure(figsize=(10, 10))
    plt.hist(df['structural_similarity'], bins=np.linspace(0, 1, 21))
    plt.yscale('log')
    # Horizontal Line at 30
    plt.axhline(30, color='black', linestyle='dashed')
    plt.text(0.9, 32, 'n=30')
    plt.ylim(10, None)
    plt.xlim(0, 1)
    plt.xlabel('Structural Similarity')
    plt.ylabel('Pair Frequency')
    plt.title("Histogram of Modified Cosine Similarity Scores")
    plt.suptitle("New Pairs Generated by Baselines_For_Benchmark")
    plt.savefig(os.path.join(output_dir, 'histogram_of_structural_similarity.png'))
    
    

def enrich_pair_dict(df, spectrum_mapping)->pd.DataFrame:
    """This function takes a dataframe with columns 'spectrum_id_1' and 'spectrum_id_2' and 
    uses the spectrum_mapping dictionary to enrich the dataframe with similarity scores."""
    
    pandarallel.initialize(progress_bar=True,nb_workers=8,use_memory_fs=None)
    
    # Define global similarity measures
    # structural_similarity_measure      = FingerprintSimilarity("jaccard")
    greedy_cosine_similarity_measure   = CosineGreedy(tolerance=0.1)
    modified_cosine_similarity_measure = ModifiedCosine(tolerance=0.5)
    
    def _computer_precursor_mass_difference(spec1, spec2)->float:
        """This function computes the precursor mass difference between two spectra."""
        return abs(spec1.get("precursor_mz") - spec2.get("precursor_mz"))
    
    def _compute_structural_similarity(spec1, spec2)->float:
        """This function computes the jaccard similarity between the fingerprints of two spectra."""
        structural_similarity_measure      = FingerprintSimilarity("jaccard")
        return structural_similarity_measure.pair(spec1, spec2)
    
    def _compute_greedy_cosine_similarity(spec1, spec2)->dict:
        """This function computes the greedy cosine similarity between the fingerprints of two spectra."""
        score = greedy_cosine_similarity_measure.pair(spec1, spec2)
        return {'greedy_cosine': score['score'], 'matched_peaks': score['matches']}
    
    def _compute_modified_cosine_similarity(spec1, spec2)->dict:
        """This function computes the modified cosine similarity between the fingerprints of two spectra."""
        score = modified_cosine_similarity_measure.pair(spec1, spec2)
        return {'modified_cosine': score['score'], 'matched_peaks': score['matches']}

    print("Enriching dataframe...", flush=True)
    # df['precursor_mass_difference'] = df.parallel_apply(lambda x: _computer_precursor_mass_difference(spectrum_mapping[x['spectrum_id_1']], spectrum_mapping[x['spectrum_id_2']]), axis=1)
    df['structural_similarity'] = df.parallel_apply(lambda x: _compute_structural_similarity(spectrum_mapping[x['spectrum_id_1']], spectrum_mapping[x['spectrum_id_2']]), axis=1)
    greedy_cosine_similarity = df.parallel_apply(lambda x: _compute_greedy_cosine_similarity(spectrum_mapping[x['spectrum_id_1']], spectrum_mapping[x['spectrum_id_2']]), axis=1)
    df['greedy_cosine'] = greedy_cosine_similarity.apply(lambda x: x['greedy_cosine'])
    modified_cosine_similarity = df.parallel_apply(lambda x: _compute_modified_cosine_similarity(spectrum_mapping[x['spectrum_id_1']], spectrum_mapping[x['spectrum_id_2']]), axis=1)
    df['modified_cosine'] = modified_cosine_similarity.apply(lambda x: x['modified_cosine'])
    df['matched_peaks'] = modified_cosine_similarity.apply(lambda x: x['matched_peaks'])
    
    # Add columns inchikey_1, inchikey_2
    df['inchikey_1'] = df['spectrum_id_1'].map(lambda x: spectrum_mapping[x].metadata['inchikey'][:14])
    df['inchikey_2'] = df['spectrum_id_2'].map(lambda x: spectrum_mapping[x].metadata['inchikey'][:14])
    
    # TEMP:
    # Remove all spectra from BMDMS-NP
    # test
    metadata = pd.read_csv('/data/nas-gpu/SourceCode/michael_s/Baselines_For_Benchmark/data/structural_similarity/raw/test_rows.csv')
    # train
    # metadata = pd.read_csv('/data/nas-gpu/SourceCode/michael_s/Baselines_For_Benchmark/data/structural_similarity/raw/train_rows.csv')
    # BDSMP_IDS = metadata.loc[metadata['GNPS_library_membership'] == 'BMDMS-NP', 'spectrum_id'].values
    
    # original_len = len(df)
    # df = df[~df['spectrum_id_1'].isin(BDSMP_IDS)]
    # df = df[~df['spectrum_id_2'].isin(BDSMP_IDS)]
    
    # print(f"Removed {original_len - len(df)} pairs from BMDMS-NP")
    
    return df

def generate_pairs(spectra:list, output_csv_path:str):
    """ This function takes a list of matchms.Spectrum objects whose metadata contains
    'ms_mass_analyzer', and 'ms_ionisation' keys. It generates a csv file with all possible
    pairs of spectra whose mass analyzer and ionisation match.
    
    Parameters:
    - spectra: list of matchms.Spectrum objects
    - output_csv_path: str, path to the output csv file
    
    Returns:
    None
    """
    
    output_lst = []
    
    filtered_by_mass_analyzer = 0
    filtered_by_ionisation = 0
    filtered_by_adduct = 0
    filtered_by_precursor_mz = 0
    filtered_by_no_ce = 0
    filtered_by_mismatched_ce = 0
    
    # TEMP
    # Get unique structures only
    # spectra = {s.metadata.get('inchikey')[:14]: s for s in spectra}
    # spectra = list(spectra.values())    
    
    # Filter spectra
    # spectra = [remove_peaks_around_precursor_mz(s, 17) for s in spectra]
    # spectra = [remove_peaks_outside_top_k(s, k=6, mz_window=25) for s in spectra]
    # spectra = [x for x in spectra if x is not None]
    spectra = [remove_peaks_around_precursor_mz(s, 0.1) for s in spectra]
    
    
    # TEMP:
    # Add collision energy to the spectra
    import numpy as np
    # test
    # metadata = pd.read_csv('/data/nas-gpu/SourceCode/michael_s/Baselines_For_Benchmark/data/structural_similarity/raw/test_rows.csv')
    # # train
    # # metadata = pd.read_csv('/data/nas-gpu/SourceCode/michael_s/Baselines_For_Benchmark/data/structural_similarity/raw/train_rows.csv')
    # for spectrum in spectra:
    #     ce = metadata.loc[metadata['spectrum_id'] == spectrum.metadata['spectrum_id'], 'collision_energy'].values[0]
    #     if not np.isnan(ce):
    #         spectrum.set('collision_energy', ce)
    #         # print(ce)
        
    # Create a dataframe with the metadata and spectrum objects
    spectrum_df = pd.DataFrame([{'spectrum_id': s.metadata['spectrum_id'],
                                'ms_mass_analyzer': s.metadata.get('ms_mass_analyzer'), 
                                'ms_ionisation': s.metadata.get('ms_ionisation'), 
                                'adduct': s.metadata['adduct'], 
                                'collision_energy': s.metadata.get('collision_energy'), 
                                'precursor_mz': s.metadata.get('precursor_mz'), 
                                } for s in spectra])                   # TODO: Remove spectrum, it's not needed 'spectrum': s
    
    spectrum_df.collision_energy = spectrum_df.collision_energy.astype('float16')
    
    # Remove everything without a ms_mass_analyzer, ms_ionisation
    print("Removing Spectra Without Mass Analyzer or Ionisation...", flush=True)
    print("Original Length: ", len(spectrum_df))
    spectrum_df = spectrum_df.dropna(subset=['ms_mass_analyzer', 'ms_ionisation'])
    print("New Length: ", len(spectrum_df))
    
    spectrum_df = spectrum_df.groupby(['ms_mass_analyzer', 'ms_ionisation', 'adduct'])
    
    # print("Generating pairs...", flush=True)
    # def generate_pair(spec1, spec2):
    #     errors = []
    #     if spec1.metadata.get('ms_mass_analyzer', -20) != spec2.metadata.get('ms_mass_analyzer', -21):
    #         errors.append("Filtered by mass analyzer")
    #     if spec1.metadata.get('ms_ionisation', -20) != spec2.metadata.get('ms_ionisation', -21):
    #         errors.append("Filtered by ionisation")
    #     if spec1.metadata.get('adduct', -20) != spec2.metadata.get('adduct', -21):
    #         errors.append("Filtered by adduct")
    #     # From "Comparison of Cosine, Modified Cosine, and Neutral Loss..."
    #     if abs(spec1.metadata['precursor_mz'] - spec2.metadata['precursor_mz']) > 200:
    #         errors.append("Filtered by precursor mz")
    #     # if spec1.metadata.get('collision_energy') is None or spec2.metadata.get('collision_energy') is None:
    #     #     errors.append("Filtered by no collision energy")
    #     # if spec1.metadata.get('collision_energy', -100) != spec2.metadata.get('collision_energy', -20000):
    #     #     errors.append("Filtered by mismatched collision energy")

    #     if errors:
    #         return ', '.join(errors)  # Return a string with the error reasons
    #     else:
    #         return {'spectrum_id_1': spec1.metadata['spectrum_id'],
    #                 'spectrum_id_2': spec2.metadata['spectrum_id'],
    #                 'mass_analyzer': spec1.metadata.get('ms_mass_analyzer', ''),
    #                 'ionisation': spec1.metadata.get('ms_ionisation','')}

    # print("Generating pairs...", flush=True)
    # # For each subset of spectra with the same mass analyzer, ionisation, and adduct, generate pairs
    # output_lst = []
    # print(f"Total Groups: {len(spectrum_df)}")
    # def _prec_mz_check(spec1, spec2):
    #     if abs(spec1.metadata['precursor_mz'] - spec2.metadata['precursor_mz']) < 200:
    #         return {'spectrum_id_1': spec1.metadata['spectrum_id'],
    #                                    'spectrum_id_2': spec2.metadata['spectrum_id'],
    #                                    'mass_analyzer': spec1.metadata.get('ms_mass_analyzer', ''),
    #                                    'ionisation': spec1.metadata.get('ms_ionisation','')}
    #     else:
    #         return None
    # for _, group in spectrum_df:
    #     pairs = [[i, j] for i in range(len(group)) for j in range(i+1, len(group))]
    #     # Split pairs into 1000 jobs to avoid excessive overhead
    #     chunks = np.array_split(pairs, len(pairs)//1000)
    #     # Get dataframe for each chunk
        
        
    #     local_list = Parallel(n_jobs=-1)(delayed(_prec_mz_check)(group.iloc[i]['spectrum'], group.iloc[j]['spectrum']) for i, j in tqdm(chunks))
    #     local_list = [x for x in local_list if x is not None]
    #     output_lst.extend(local_list)
        
    print("Generating pairs...", flush=True)

    # For each subset of spectra with the same mass analyzer, ionisation, and adduct, generate pairs
    output_lst = []
    print(f"Total Groups: {len(spectrum_df)}")
    
    for _, group in tqdm(spectrum_df):
        # print(group)
        # Get all pairs with precursor mz difference less than 200 using vectorized operations
        pairs = group.merge(group, on=['ms_mass_analyzer', 'ms_ionisation', 'adduct'], suffixes=('_1', '_2'))   # Will perform outer product
        pairs = pairs.loc[pairs['spectrum_id_1'] < pairs['spectrum_id_2']]                                      # Get upper diagonal
        pairs['precursor_mass_difference'] = (pairs['precursor_mz_1'] - pairs['precursor_mz_2']).abs().astype('float16')
        pairs = pairs.loc[abs(pairs['precursor_mz_1'] - pairs['precursor_mz_2']) < 200]
        # If they have collision energy, filter it out if the difference is greater than 5, if they don't just let them through
        # DEBUG TO SEE IF WE CAN JUST ONLY USE COLLISION ENERGY PAIRS
        org_len = len(pairs)
        ce_pairs    = pairs.loc[(pairs['collision_energy_1'].notna() & pairs['collision_energy_2'].notna())]
        ce_pairs    = ce_pairs.loc[(pairs['collision_energy_1'] - ce_pairs['collision_energy_2']).abs() <= 5]
        other_pairs = pairs.loc[pairs['collision_energy_1'].isna() | pairs['collision_energy_2'].isna()]
        pairs = pd.concat((ce_pairs, other_pairs))
        # pairs = ce_pairs
        new_len = len(pairs)
        filtered_by_mismatched_ce += org_len - new_len
        del ce_pairs, other_pairs
        
        output_lst.extend(pairs[['spectrum_id_1', 'spectrum_id_2', 'ms_mass_analyzer', 'ms_ionisation', 'precursor_mass_difference',]].to_dict('records'))
        del pairs
        gc.collect()
    
    del spectrum_df
    gc.collect()
    
    # print(f"Filtered by mass analyzer: {filtered_by_mass_analyzer}")
    # print(f"Filtered by ionisation: {filtered_by_ionisation}")
    # print(f"Filtered by adduct: {filtered_by_adduct}")
    # print(f"Filtered by precursor mz: {filtered_by_precursor_mz}")
    # print(f"Filtered by no collision energy: {filtered_by_no_ce}")
    print(f"Filtered by mismatched collision energy: {filtered_by_mismatched_ce}")
    
    columns=['spectrum_id_1', 'spectrum_id_2', 'ms_mass_analyzer', 'ms_ionisation', 'precursor_mass_difference']
    output_df = pd.DataFrame(output_lst, columns=columns)
    del output_lst
    gc.collect()
    
    # Prune pairs
    # We want to keep one spectrum id for each structure so we'll count take one with the highest pair count
    print("Pruning pairs...", flush=True)
    org_len = len(output_df)
    spectra = [derive_inchi_from_smiles(s) for s in spectra]
    spectra = [derive_inchikey_from_inchi(s) for s in spectra]
    spectrum_inchikey14_mapping = {s.metadata['spectrum_id']: s.metadata['inchikey'][:14] for s in spectra}
    
    all_spectrum_ids = pd.concat((output_df['spectrum_id_1'], output_df['spectrum_id_2']))
    spectrum_counts = all_spectrum_ids.value_counts()
    print(spectrum_counts)
    spectrum_counts = spectrum_counts.reset_index()
    print(spectrum_counts)
    spectrum_counts.columns = ['spectrum_id', 'count']
    spectrum_counts['inchikey_14'] = spectrum_counts['spectrum_id'].map(spectrum_inchikey14_mapping)
    spectrum_counts = spectrum_counts.groupby('inchikey_14').apply(lambda x: x.sort_values('count', ascending=False).iloc[0])
    
    print(spectrum_counts)
    
    output_df = output_df.loc[output_df['spectrum_id_1'].isin(spectrum_counts['spectrum_id']) & output_df['spectrum_id_2'].isin(spectrum_counts['spectrum_id'])]
    print(f"Pruned {org_len - len(output_df)} pairs", flush=True)
    del spectrum_counts, all_spectrum_ids, spectrum_inchikey14_mapping
    gc.collect()
    
    # Before enrichment, add fingerprints to the spectra
    print("Adding fingerprints to spectra...", flush=True)
    spectra = [add_fingerprint(s) for s in spectra]

    spectrum_mapping = {s.metadata['spectrum_id']: s for s in spectra}
    for s in spectrum_mapping.values():
        assert isinstance(s, Spectrum)
    
    output_df = enrich_pair_dict(output_df, spectrum_mapping)
    
    print("Saving pairs...", flush=True)
    output_df.to_csv(output_csv_path, index=False)
    

def main():
    parser = parser = argparse.ArgumentParser(description='Generate valid pairs')
    parser.add_argument('--input_pickle_path', type=str, help='Input file')
    parser.add_argument('--output_csv_path', type=str, help='Output file')
    parser.add_argument('--summary_plot_dir', type=str, help='Path to the directory where summary plots will be saved', default=None)
    args = parser.parse_args()
    
    print("Loading spectra...", flush=True)
    if args.input_pickle_path.endswith('.pickle'):
        spectra = pickle.load(open(args.input_pickle_path, 'rb'))
    elif args.input_pickle_path.endswith('.mgf'):
        from matchms.importing import load_from_mgf

        spectra = list(load_from_mgf(args.input_pickle_path))
    else:
        raise ValueError("Input file must be a pickle or mgf file")
    
    generate_pairs(spectra, args.output_csv_path)
    
    if not os.path.exists(args.summary_plot_dir):
        os.makedirs(args.summary_plot_dir, exist_ok=True)
    
    if args.summary_plot_dir is not None:
        produce_summary_plots(args.output_csv_path, args.summary_plot_dir)
        

if __name__ == "__main__":
    gc.enable()
    main()