import argparse
import os
import gc
import pickle
import pandas as pd
import numpy as np
from matchms.Spectrum import Spectrum
from matchms.filtering.metadata_processing.add_fingerprint import add_fingerprint
from matchms.similarity import FingerprintSimilarity, ModifiedCosine, CosineGreedy
from matchms.filtering import remove_peaks_around_precursor_mz, remove_peaks_outside_top_k
from matchms.filtering.metadata_processing.derive_inchi_from_smiles import derive_inchi_from_smiles 
from matchms.filtering.metadata_processing.derive_inchikey_from_inchi import derive_inchikey_from_inchi
from pandarallel import pandarallel
from parallel_pandas import ParallelPandas
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
from joblib import Parallel, delayed

def produce_summary_plots(feather_path, output_dir, no_cosine=False):
    """This function summarizes the pairs generated by the generate_pairs function."""
    
    df = pd.read_feather(feather_path)
    
    if not no_cosine:
        # Plot the correlation between modified cosine and structural similarity
        plt.figure(figsize=(10, 10))
        plt.scatter(df['modified_cosine'], df['structural_similarity'])
        plt.xlabel('Modified Cosine')
        plt.ylabel('Structural Similarity')
        plt.title('Modified Cosine vs Structural Similarity')
        plt.savefig(os.path.join(output_dir, 'modified_cosine_vs_structural_similarity.png'))
        
        # Plot the correlation between greedy cosine and structural similarity
        plt.figure(figsize=(10, 10))
        plt.scatter(df['greedy_cosine'], df['structural_similarity'])
        plt.xlabel('Greedy Cosine')
        plt.ylabel('Structural Similarity')
        plt.title('Greedy Cosine vs Structural Similarity')
        plt.savefig(os.path.join(output_dir, 'greedy_cosine_vs_structural_similarity.png'))
        
        # 3D Plot of precursor mass difference, modified cosine and structural similarity
        fig = px.scatter_3d(df, x='precursor_mass_difference', y='modified_cosine', z='structural_similarity', color='precursor_mass_difference')
        fig.update_layout(
            scene=dict(
                xaxis_title='Precursor Mass Difference',
                yaxis_title='Modified Cosine',
                zaxis_title='Structural Similarity',
                )
        )
        fig.write_image(os.path.join(output_dir, 'precursor_mass_difference_vs_modified_cosine_vs_structural_similarity.png'))
        fig.write_html(os.path.join(output_dir, 'precursor_mass_difference_vs_modified_cosine_vs_structural_similarity.html'))
        fig.show()
        
        # 3D Plot of precursor mass difference, modified cosine, and matched peaks
        fig = px.scatter_3d(df, x='structural_similarity', y='modified_cosine', z='matched_peaks', color='structural_similarity')
        fig.update_layout(
            scene=dict(
                xaxis_title='Structural Similarity',
                yaxis_title='Modified Cosine',
                zaxis_title='Matched Peaks',
                )
        )
        fig.write_image(os.path.join(output_dir, 'structural_similarity_vs_modified_cosine_vs_matched_peaks.png'))
        fig.write_html(os.path.join(output_dir, 'structural_similarity_vs_modified_cosine_vs_matched_peaks.html'))
    
        # Plot modified cosine vs structural similarity,
        # Require 6 matches peaks
        df_copy = df.copy(deep=True)
        df_copy.loc[df_copy['matched_peaks'] < 6, 'modified_cosine'] = 0
        plt.figure(figsize=(10, 10))
        plt.scatter(df_copy['modified_cosine'], df_copy['structural_similarity'], alpha=0.10)
        # Plot y=x line
        plt.plot([0, 1], [0, 1], color='black')
        plt.xlabel('Modified Cosine')
        plt.ylabel('Structural Similarity')
        plt.title('Modified Cosine vs Structural Similarity (Matched Peaks >= 6, Modified Cosine Tol=0.5)')
        plt.savefig(os.path.join(output_dir, 'modified_cosine_vs_structural_similarity_matched_peaks_6.png'))
    
        # Create the     plot as shown in "Comparison of Cosine, Modified Cosine, and Neutral Loss Based Spectrum Alignment 
        # For Discovery ofStructurally Related Molecules"
        # Bin the data by structural similarity
        bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
        labels = ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']
        df['structural_similarity_bins'] = pd.cut(df['structural_similarity'], bins=bins, labels=labels)
        
        def _adjacent_values(vals, q1, q3):
            upper_adjacent_value = q3 + (q3 - q1) * 1.5
            print("upper_adjacent_value", upper_adjacent_value)
            print(vals[-1])
            upper_adjacent_value = np.clip(upper_adjacent_value, q3, vals[-1])

            lower_adjacent_value = q1 - (q3 - q1) * 1.5
            lower_adjacent_value = np.clip(lower_adjacent_value, vals[0], q1)
            return lower_adjacent_value, upper_adjacent_value
        
        data = [df.loc[df['structural_similarity_bins'] == i, 'modified_cosine'] for i in labels]
        data = [sorted(d) for d in data]
        
        # Create the violin plot
        plt.figure(figsize=(10, 10))
        plot = plt.violinplot(data, showmeans=False, showextrema=False, )
        for pc in plot['bodies']:
            pc.set_facecolor('blue')
            pc.set_edgecolor('black')
            pc.set_alpha(0.5)
            
        quartile1, medians, quartile3 = [],[],[]
        for lst in data:
            q1, m, q2 = np.percentile(lst, [25, 50, 75])
            quartile1.append(q1)
            medians.append(m)
            quartile3.append(q2)
            
        print(quartile1, medians, quartile3)
        print(len(quartile1), len(medians), len(quartile3), len(data))

        whiskers = np.array([ _adjacent_values(sorted_array, q1, q3) for sorted_array, q1, q3 in zip(data, quartile1, quartile3) ])
        whiskers_min, whiskers_max = whiskers[:, 0], whiskers[:, 1]

        inds = np.arange(1, len(medians) + 1)
        plt.scatter(inds, medians, marker='o', color='white', s=30, zorder=3)
        plt.vlines(inds, quartile1, quartile3, color='k', linestyle='-', lw=5)
        plt.vlines(inds, whiskers_min, whiskers_max, color='k', linestyle='-', lw=1)
            
        plt.xticks(range(1, len(labels)+1), labels)
        plt.xlabel('Structural Similarity')
        plt.ylabel('Modified Cosine')
        plt.title('Modified Cosine vs Structural Similarity')
        plt.savefig(os.path.join(output_dir, 'modified_cosine_vs_structural_similarity_violin.png'))
    
    # # Create the violin plot
    # plt.figure(figsize=(10, 10))
    # plt.violinplot([df.loc[df['structural_similarity_bins'] == i, 'modified_cosine'] for i in labels], showmeans=True)
    # plt.xticks(range(1, len(labels)+1), labels)
    # plt.xlabel('Structural Similarity')
    # plt.ylabel('Modified Cosine')
    # plt.title('Modified Cosine vs Structural Similarity')
    # plt.savefig(os.path.join(output_dir, 'modified_cosine_vs_structural_similarity_violin.png'))
    
    # Plot a histogram of the structural similarity
    plt.figure(figsize=(10, 10))
    plt.hist(df['structural_similarity'], bins=np.linspace(0, 1, 21))
    plt.yscale('log')
    # Horizontal Line at 30
    plt.axhline(30, color='black', linestyle='dashed')
    plt.text(0.9, 32, 'n=30')
    plt.ylim(10, None)
    plt.xlim(0, 1)
    plt.xlabel('Structural Similarity')
    plt.ylabel('Pair Frequency')
    plt.title("Histogram of Structural Similarity Scores")
    plt.suptitle("New Pairs Generated by Baselines_For_Benchmark")
    plt.savefig(os.path.join(output_dir, 'histogram_of_structural_similarity.png'))
    
     

def enrich_pair_dict(df, spectrum_mapping, no_cosine=False)->pd.DataFrame:
    """This function takes a dataframe with columns 'spectrum_id_1' and 'spectrum_id_2' and 
    uses the spectrum_mapping dictionary to enrich the dataframe with similarity scores."""
    
    # pandarallel.initialize(progress_bar=False,nb_workers=8,use_memory_fs=None)
    ParallelPandas.initialize(n_cpu=min(os.cpu_count(),8), split_factor=24, disable_pr_bar=False)
    
    # Define global similarity measures
    # structural_similarity_measure      = FingerprintSimilarity("jaccard")
    greedy_cosine_similarity_measure   = CosineGreedy(tolerance=0.1)
    modified_cosine_similarity_measure = ModifiedCosine(tolerance=0.5)
    
    def _computer_precursor_mass_difference(spec1, spec2)->float:
        """This function computes the precursor mass difference between two spectra."""
        return abs(spec1.get("precursor_mz") - spec2.get("precursor_mz"))
    
    def _compute_structural_similarity(spec1, spec2)->float:
        """This function computes the jaccard similarity between the fingerprints of two spectra."""
        structural_similarity_measure      = FingerprintSimilarity("jaccard")
        return structural_similarity_measure.pair(spec1, spec2)
    
    def _compute_greedy_cosine_similarity(spec1, spec2)->dict:
        """This function computes the greedy cosine similarity between the fingerprints of two spectra."""
        score = greedy_cosine_similarity_measure.pair(spec1, spec2)
        return score['score'].astype('float16'), score['matches']
    
    def _compute_modified_cosine_similarity(spec1, spec2)->dict:
        """This function computes the modified cosine similarity between the fingerprints of two spectra."""
        score = modified_cosine_similarity_measure.pair(spec1, spec2)
        return score['score'].astype('float16'), score['matches']

    print("Enriching dataframe...", flush=True)
    
    df['structural_similarity'] = df.p_apply(lambda x: _compute_structural_similarity(spectrum_mapping[x['spectrum_id_1']], spectrum_mapping[x['spectrum_id_2']]), axis=1).astype('float16')
    # Add columns inchikey_1, inchikey_2
    df['inchikey_1'] = df['spectrum_id_1'].map(lambda x: spectrum_mapping[x].metadata['inchikey'][:14]).astype('category')
    df['inchikey_2'] = df['spectrum_id_2'].map(lambda x: spectrum_mapping[x].metadata['inchikey'][:14]).astype('category')
    if no_cosine:
        print("Skipping cosine similarity computation")
        return df
    
    greedy_cosine_similarity = df.p_apply(lambda x: _compute_greedy_cosine_similarity(spectrum_mapping[x['spectrum_id_1']], spectrum_mapping[x['spectrum_id_2']]), axis=1)
    df['greedy_cosine'] = greedy_cosine_similarity.p_apply(lambda x: x[0]).astype('float16')
    modified_cosine_similarity = df.p_apply(lambda x: _compute_modified_cosine_similarity(spectrum_mapping[x['spectrum_id_1']], spectrum_mapping[x['spectrum_id_2']]), axis=1)
    df['modified_cosine'] = modified_cosine_similarity.p_apply(lambda x: x[0]).astype('float16')
    df['matched_peaks'] = modified_cosine_similarity.p_apply(lambda x: x[1])
        
    return df

    # TEMP:
    # Remove all spectra from BMDMS-NP
    # test
    # metadata = pd.read_feather('/data/nas-gpu/SourceCode/michael_s/Baselines_For_Benchmark/data/structural_similarity/raw/test_rows.feather')
    # train
    # metadata = pd.read_feather('/data/nas-gpu/SourceCode/michael_s/Baselines_For_Benchmark/data/structural_similarity/raw/train_rows.feather')
    # BDSMP_IDS = metadata.loc[metadata['GNPS_library_membership'] == 'BMDMS-NP', 'spectrum_id'].values
    
    # original_len = len(df)
    # df = df[~df['spectrum_id_1'].isin(BDSMP_IDS)]
    # df = df[~df['spectrum_id_2'].isin(BDSMP_IDS)]
    
    # print(f"Removed {original_len - len(df)} pairs from BMDMS-NP")
    
    # return df

def reduce_spectra(spectrum_df, pre_filter_count):
    """
    Reduces the number of spectra in the given DataFrame to a specified count per inchikey.

    Parameters:
    spectrum_df (pd.DataFrame): The DataFrame containing the spectra data. Must have a column named 'inchikey'.
    pre_filter_count (int): The maximum number of spectra to keep per inchikey. Must be a positive integer.

    Returns:
    pd.DataFrame: The filtered DataFrame with at most `pre_filter_count` spectra per 'inchikey'.
    
    Notes:
    - If `pre_filter_count` is None, the function will return the original DataFrame.
    - If `pre_filter_count` is not a positive integer, the function will print an error message and return the original DataFrame.
    - The function prints the original and new lengths of the DataFrame before and after filtering.
    - If the filtering operation fails, the function will print an error message and return the original DataFrame.
    - If the filtering operation does not reduce the number of spectra, a warning message will be printed.
    """
    if pre_filter_count is not None:
        if not isinstance(pre_filter_count, int) or pre_filter_count <= 0:
            print("Error: pre_filter_count must be a positive integer", flush=True)
            return spectrum_df

        print(f"Reducing the number of spectra to {pre_filter_count} per inchikey...", flush=True)
        original_length = len(spectrum_df)
        print(f"Original Length: {original_length}", flush=True)

        try:
            spectrum_df = spectrum_df.groupby('inchikey', observed=False).head(pre_filter_count).reset_index(drop=True)
        except Exception as e:
            print(f"Error during filtering: {e}", flush=True)
            return spectrum_df

        new_length = len(spectrum_df)
        print(f"New Length: {new_length}", flush=True)
        
        if original_length == new_length:
            print("Warning: The filtering did not reduce the number of spectra.", flush=True)
        
    return spectrum_df

def generate_pairs(spectra:list, 
                   output_feather_path:str, 
                   prune:bool=False, 
                   no_cosine:bool=False, 
                   merge_on_lst:list=None,
                   mass_analyzer_lst:list=None,
                   collision_energy_thresh:float=5.0,
                   skip_precursor_mz=False,
                   pre_filter_count=None)->None:
    """ This function takes a list of matchms.Spectrum objects whose metadata contains
    'ms_mass_analyzer', and 'ms_ionisation' keys. It generates a feather file with all possible
    pairs of spectra whose mass analyzer and ionisation match.
    
    Parameters:
    - spectra: list of matchms.Spectrum objects
    - output_feather_path: str, path to the output feather file
    - prune: bool, whether to remove duplicate structures
    - merge_on_lst: list of str, the criteria to filter pairs by. Can include 'adduct', 'ms_mass_analyzer', and 'ms_ionisation'.
                    Defaults to None, in which case all three are used.
    - mass_analyzer_lst: list of str, required mass analyzer to be included in output (e.g., 'tof', 'orbitrap', and 'qtof'). 
                            Defaults to None, in which case all mass analyzers are allowed.
    - collision_energy_thresh: float, maximum difference in collision energy to be considered a pair. If not collision energy is available
                                for either spectrum, the pair is included. If "-1.0" is specified, the criteria is no considered. Default is 5.0.
    - skip_precursor_mz: bool, whether to skip the precursor mz requirement. Default is False.
    
    Returns:
    None
    """
    
    output_lst = []
    
    filtered_by_mass_analyzer = 0
    filtered_by_ionisation = 0
    filtered_by_adduct = 0
    filtered_by_precursor_mz = 0
    filtered_by_no_ce = 0
    filtered_by_mismatched_ce = 0
    
    if merge_on_lst is not None:
        for x in merge_on_lst:
            assert x in ['ms_mass_analyzer', 'ms_ionisation', 'adduct', 'library']
    else:
        merge_on_lst = ['ms_mass_analyzer', 'ms_ionisation', 'adduct']
        
    print("Using the following criteria for pairs:", merge_on_lst)

    if mass_analyzer_lst is not None:
        mass_analyzer_lst = set(mass_analyzer_lst)
        print("Only including the following mass analyzers", mass_analyzer_lst)
    
    # TEMP
    # Get unique structures only
    # spectra = {s.metadata.get('inchikey')[:14]: s for s in spectra}
    # spectra = list(spectra.values())    
    
    # Filter spectra for Cosine
    # spectra = [remove_peaks_around_precursor_mz(s, 17) for s in spectra]
    # spectra = [remove_peaks_outside_top_k(s, k=6, mz_window=25) for s in spectra]
    # spectra = [x for x in spectra if x is not None]
    # spectra = [remove_peaks_around_precursor_mz(s, 0.1) for s in spectra]
        
    # Create a dataframe with the metadata and spectrum objects
    spectrum_df = pd.DataFrame([{'spectrum_id': s.metadata['spectrum_id'],
                                'ms_mass_analyzer': s.metadata.get('ms_mass_analyzer'), 
                                'ms_ionisation': s.metadata.get('ms_ionisation'), 
                                'adduct': s.metadata['adduct'], 
                                'collision_energy': s.metadata.get('collision_energy'), 
                                'precursor_mz': s.metadata.get('precursor_mz'), 
                                'library': s.metadata.get('library_membership'),
                                'inchikey': s.metadata.get('inchikey')[:14],
                                } for s in spectra],)
    
    if 'library' not in merge_on_lst:
        # Drop the column to save memory
        spectrum_df = spectrum_df.drop(columns=['library'])
    else:
        # Cast to category to save memory
        spectrum_df.library = spectrum_df.library.astype('category')
    
    spectrum_df.collision_energy = spectrum_df.collision_energy.astype('float16')

    # Remove everything without a ms_mass_analyzer, ms_ionisation
    if 'ms_mass_analyzer' in merge_on_lst:
        print("Removing Spectra Without Mass Analyzer...", flush=True)
        print("Original Length: ", len(spectrum_df))
        spectrum_df = spectrum_df.dropna(subset=['ms_mass_analyzer'])
        print("New Length: ", len(spectrum_df))
    if 'ms_ionisation' in merge_on_lst:
        print("Removing Spectra Without Ionisation...", flush=True)
        print("Original Length: ", len(spectrum_df))
        spectrum_df = spectrum_df.dropna(subset=['ms_ionisation'])
        print("New Length: ", len(spectrum_df))
    
    print(spectrum_df)
    # Reduces training set to specified spectrum count per inchikey
    spectrum_df = reduce_spectra(spectrum_df, pre_filter_count)
    print(spectrum_df)
    
    # Cast columns to categorical to save memory
    spectrum_df['adduct'] = spectrum_df['adduct'].astype('category')
    spectrum_df['ms_mass_analyzer'] = spectrum_df['ms_mass_analyzer'].str.lower().astype('category')    # Set mass analyzer to lower for filtering
    spectrum_df['ms_ionisation'] = spectrum_df['ms_ionisation'].astype('category')
    spectrum_df['inchikey'] = spectrum_df['inchikey'].astype('category')
    
    # Optionally filter by ms_mass_analyzer
    if mass_analyzer_lst is not None:
        print("Filtering by mass analyzer. Starting Value Counts:")
        print(spectrum_df.ms_mass_analyzer.value_counts())
        print("Current Spectrum Count:", len(spectrum_df))
        spectrum_df = spectrum_df.loc[spectrum_df['ms_mass_analyzer'].isin(mass_analyzer_lst)]
        print("Value Counts After Filtration:")
        print(spectrum_df.ms_mass_analyzer.value_counts())
        print("New Spectrum Count:", len(spectrum_df))

    spectrum_df = spectrum_df.groupby(merge_on_lst, observed=False)
        
    print("Generating pairs...", flush=True)

    # For each subset of spectra with the same mass analyzer, ionisation, and adduct, generate pairs
    output_lst = []
    print(f"Total Groups: {len(spectrum_df)}")
    
    if collision_energy_thresh == -1.0:
        print("Skipping collision energy criteria")
    if skip_precursor_mz:
        print("Skipping precursor mz requirement")
    
    ## TODO: To save memory, we can write the spectrum_df to a temporary hdfstore and read one group at a time
    ## TODO: Do this per inchikey?

    for _, group in tqdm(spectrum_df):
        # print(group)
        # Get all pairs with precursor mz difference less than 200 using vectorized operations
        # Tile over the outer join to get all pairs (without using too much memory)
        patch_count = 50
        patch_size = int(len(group)/patch_count)
        print("Patch Size:", patch_size, flush=True)
        for subgroup_idx in tqdm(range(patch_count)):
            if subgroup_idx < patch_count - 1:
                subgroup = group.iloc[subgroup_idx*patch_size:(subgroup_idx+1)*patch_size]
            else:
                subgroup = group.iloc[subgroup_idx*patch_size:]
            if len(subgroup) == 0:
                # For a set of spectra with length < int(len(group)/patch_count), the first patch_count-1 iterations will be empty, skip them
                continue

            pairs = group.merge(subgroup, on=merge_on_lst, suffixes=('_1', '_2'))   # Will perform outer product                   # TODO: Reza's idea: tile over this
            pairs = pairs.loc[pairs['spectrum_id_1'] < pairs['spectrum_id_2']]   # Get upper diagonal
            
            org_len = len(pairs)
            if not skip_precursor_mz:
                pairs['precursor_mass_difference'] = (pairs['precursor_mz_1'] - pairs['precursor_mz_2']).abs().astype('float16')
                pairs = pairs.loc[abs(pairs['precursor_mz_1'] - pairs['precursor_mz_2']) < 200]
                filtered_by_precursor_mz += org_len - len(pairs)
            # If they have collision energy, filter it out if the difference is greater than 5, if they don't just let them through
            if collision_energy_thresh != -1.0:
                org_len = len(pairs)
                ce_pairs    = pairs.loc[(pairs['collision_energy_1'].notna() & pairs['collision_energy_2'].notna())]
                ce_pairs    = ce_pairs.loc[(pairs['collision_energy_1'] - ce_pairs['collision_energy_2']).abs() <= collision_energy_thresh]
                other_pairs = pairs.loc[pairs['collision_energy_1'].isna() | pairs['collision_energy_2'].isna()]
                pairs = pd.concat((ce_pairs, other_pairs))
                new_len = len(pairs)
                filtered_by_mismatched_ce += org_len - new_len
                del ce_pairs, other_pairs
            
            # If we have ms_mass_analyzer_1 and ms_mass_analyzer_2, concatenate them with a semicolon
            if 'ms_mass_analyzer_1' in pairs.columns and 'ms_mass_analyzer_2' in pairs.columns:
                pairs['ms_mass_analyzer'] = pairs[['ms_mass_analyzer_1', 'ms_mass_analyzer_2']].apply(lambda x: str(x.iloc[0]) + ';' + str(x.iloc[1]), axis=1)
                pairs['ms_mass_analyzer'] = pairs['ms_mass_analyzer'].astype('category')
            # If we have ms_ionisation_1 and ms_ionisation_2, concatenate them with a semicolon
            if 'ms_ionisation_1' in pairs.columns and 'ms_ionisation_2' in pairs.columns:
                pairs['ms_ionisation'] = pairs[['ms_ionisation_1', 'ms_ionisation_2']].apply(lambda x: str(x.iloc[0]) + ';' + str(x.iloc[1]), axis=1)
                pairs['ms_ionisation'] = pairs['ms_ionisation'].astype('category')

            output_lst.extend(pairs[['spectrum_id_1', 'spectrum_id_2', 'ms_mass_analyzer', 'ms_ionisation', 'precursor_mass_difference','inchikey_1','inchikey_2']].to_dict('records'))
            columns=['spectrum_id_1', 'spectrum_id_2', 'ms_mass_analyzer', 'ms_ionisation', 'precursor_mass_difference']
            
            ############ TEMP OUTPUT FOR DEBUGGING ############
            # temp_output_df = pd.DataFrame(output_lst, columns=columns)

            # if len(temp_output_df) > 0:
            #     pretty_name = ''
            #     # For each of the merge on criteria, add the unique value of that column to the string
            #     for merge_on in merge_on_lst:
            #         pretty_name += f"{pairs[merge_on].iloc[0]}_"

            #     pretty_name = '_' + pretty_name + "_"
            #     temp_output_df.to_csv(f'{output_feather_path.rsplit(".", 1)[0]}{pretty_name}{subgroup_idx}.csv')
            # else:
            #     print("\nNo pairs to save")

            # output_lst = []
            ####################################################
            
            del pairs
            gc.collect()
    
    del spectrum_df
    gc.collect()
    
    # print(f"Filtered by mass analyzer: {filtered_by_mass_analyzer}")
    # print(f"Filtered by ionisation: {filtered_by_ionisation}")
    # print(f"Filtered by adduct: {filtered_by_adduct}")
    # print(f"Filtered by precursor mz: {filtered_by_precursor_mz}")
    # print(f"Filtered by no collision energy: {filtered_by_no_ce}")
    print(f"Filtered by mismatched collision energy: {filtered_by_mismatched_ce}")
    
    columns=['spectrum_id_1', 'spectrum_id_2', 'ms_mass_analyzer', 'ms_ionisation', 'precursor_mass_difference']
    output_df = pd.DataFrame(output_lst, columns=columns)
    output_df['spectrum_id_1'] = output_df['spectrum_id_1'].astype('category')
    output_df['spectrum_id_2'] = output_df['spectrum_id_2'].astype('category')
    output_df['ms_mass_analyzer'] = output_df['ms_mass_analyzer'].astype('category')
    output_df['ms_ionisation'] = output_df['ms_ionisation'].astype('category')
    output_df['precursor_mass_difference'] = output_df['precursor_mass_difference'].astype('float16')
    
    del output_lst
    gc.collect()
    
    # Prune pairs
    # We want to keep a few spectrum id for each structure so we'll count take ones with the highest pair count
    if prune:
        print("Pruning pairs...", flush=True)
        org_len = len(output_df)
        spectra = [derive_inchi_from_smiles(s) for s in spectra]
        spectra = [derive_inchikey_from_inchi(s) for s in spectra]
        spectrum_inchikey14_mapping = {s.metadata['spectrum_id']: s.metadata['inchikey'][:14] for s in spectra}
        
        all_spectrum_ids = pd.concat((output_df['spectrum_id_1'], output_df['spectrum_id_2']))
        spectrum_counts = all_spectrum_ids.value_counts()
        print(spectrum_counts)
        spectrum_counts = spectrum_counts.reset_index()
        print(spectrum_counts)
        spectrum_counts.columns = ['spectrum_id', 'count']
        spectrum_counts['inchikey_14'] = spectrum_counts['spectrum_id'].map(spectrum_inchikey14_mapping)
        spectrum_counts = spectrum_counts.groupby('inchikey_14').apply(lambda x: x.sort_values('count', ascending=False).iloc[0:prune])
        
        print(spectrum_counts)
        
        output_df = output_df.loc[output_df['spectrum_id_1'].isin(spectrum_counts['spectrum_id']) & output_df['spectrum_id_2'].isin(spectrum_counts['spectrum_id'])]
        print(f"Pruned {org_len - len(output_df)} pairs", flush=True)
        del spectrum_counts, all_spectrum_ids, spectrum_inchikey14_mapping
        gc.collect()
    
    # Before enrichment, add fingerprints to the spectra
    print("Adding fingerprints to spectra...", flush=True)
    spectra = [add_fingerprint(s) for s in spectra]

    spectrum_mapping = {s.metadata['spectrum_id']: s for s in spectra}
    for s in spectrum_mapping.values():
        assert isinstance(s, Spectrum)
    
    output_df = enrich_pair_dict(output_df, spectrum_mapping, no_cosine)
    
    print("Saving pairs...", flush=True)
    output_df.to_feather(output_feather_path)
    

def main():
    parser = parser = argparse.ArgumentParser(description='Generate valid pairs')
    parser.add_argument('--input_pickle_path', type=str, help='Input file') # Contains all spectra in the dataset
    parser.add_argument('--output_feather_path', type=str, help='Output file')  # How to output the pairs
    parser.add_argument('--summary_plot_dir', type=str, help='Path to the directory where summary plots will be saved', default=None)
    parser.add_argument('--prune_duplicate_structures', type=int, help='Prune duplicate structures', default=0)
    parser.add_argument('--pre_filter_count', type=int, help='Maximum count of spectra per structure.', default=None)
    parser.add_argument('--no_cosine', action='store_true', help='Do not compute cosine similarity', default=False)
    parser.add_argument('--merge_on_lst', type=str, help='A semicolon delimited list of criteria to merge on. \
                                                            Options are: ["ms_mass_analyzer", "ms_ionisation", "adduct", "library"]',
                                                    default=None)
    # To facilitate qtof/tof-orbitrap pairs only
    parser.add_argument('--mass_analyzer_lst', type=str, default=None,
                                                help='A semicolon delimited list of allowed mass analyzers. All mass analyzers are \
                                                      allowed when not specified')
    # To facilitate no collision energy (CE) criteria
    parser.add_argument('--collision_energy_thresh', type=float, default=5.0,
                                                help='The maximum difference between collision energies of two spectra to be considered\
                                                    a pair. Default is <= 5. "-1.0" means collision energies are not filtered. \
                                                    If no collision enegy is available for either spectra, both will be included.')
    parser.add_argument('--no_pm_requirement', action='store_true', help='Do not require precursor mass difference to be less than 200', default=False)
    
    args = parser.parse_args()
    
    print("Loading spectra...", flush=True)
    if args.input_pickle_path.endswith('.pickle'):
        spectra = pickle.load(open(args.input_pickle_path, 'rb'))
    elif args.input_pickle_path.endswith('.mgf'):
        from matchms.importing import load_from_mgf

        spectra = list(load_from_mgf(args.input_pickle_path))
    else:
        raise ValueError("Input file must be a pickle or mgf file")
    
    if args.merge_on_lst is not None:
        merge_on_lst = args.merge_on_lst.split(';')
        merge_on_lst = [str(x).strip().lower() for x in merge_on_lst]
    else:
        merge_on_lst = None
    
    if args.mass_analyzer_lst is not None:
        mass_analyzer_lst = args.mass_analyzer_lst.split(';')
        mass_analyzer_lst = [str(x).strip().lower() for x in mass_analyzer_lst]
    else:
        mass_analyzer_lst = None
    
    collision_energy_thresh = float(args.collision_energy_thresh)
    
    generate_pairs(spectra, args.output_feather_path,
                   prune=args.prune_duplicate_structures,
                   no_cosine=args.no_cosine,
                   merge_on_lst=merge_on_lst,
                   mass_analyzer_lst=mass_analyzer_lst,
                   collision_energy_thresh=collision_energy_thresh,
                   skip_precursor_mz=args.no_pm_requirement,
                   pre_filter_count=args.pre_filter_count)
    
    if args.summary_plot_dir is not None:
        if not os.path.exists(args.summary_plot_dir):
            os.makedirs(args.summary_plot_dir, exist_ok=True)
        produce_summary_plots(args.output_feather_path, args.summary_plot_dir, args.no_cosine)
        

if __name__ == "__main__":
    gc.enable()
    main()